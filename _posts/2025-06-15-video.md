---
layout: post
title: Video｜视频生成模型技术进展
categories: [Video]
description: 视频生成模型技术进展
keywords: Video, Seedance, Veo, Sora, Hunyuan, Magi-1, LTX-Video, BAGEL, Kling, Wanxiang
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
topmost: false
---


- [一、头部商业模型](#一头部商业模型)
  - [字节跳动 Seedance 1.0 Pro](#字节跳动-seedance-10-pro)
  - [谷歌 Veo 3](#谷歌-veo-3)
  - [OpenAI Sora](#openai-sora)
- [二、国产模型生态](#二国产模型生态)
  - [快手可灵 2.1](#快手可灵-21)
  - [腾讯 Hunyuan](#腾讯-hunyuan)
  - [阿里通义万相](#阿里通义万相)
- [三、开源与学术突破](#三开源与学术突破)
  - [清华团队 Magi-1](#清华团队-magi-1)
  - [LTX-Video](#ltx-video)
  - [BAGEL（字节跳动）](#bagel字节跳动)
- [四、技术趋势与挑战](#四技术趋势与挑战)
  - [生成质量提升](#生成质量提升)
  - [成本降低](#成本降低)
  - [伦理争议](#伦理争议)


近年来，视频生成模型技术快速发展，国内外企业和研究机构推出多款具有突破性能力的模型。头部商务模型有字节跳动 Seedance 1.0 Pro、Veo 3、Sora模型。国产模型生态有快手可灵 2.1、腾讯 Hunyuan、阿里通义万相模型。开源与学术突有清华团队 Magi-1、LTX-Video、BAGEL（字节跳动）等模型。


近年来，视频生成模型技术快速发展，国内外企业和研究机构推出了多款具有突破性能力的模型，主要进展包括：





## 一、头部商业模型

### 字节跳动 Seedance 1.0 Pro

该模型在文生视频、图生视频任务中表现全球领先，支持多镜头无缝切换的1080P视频生成，成本为行业最低。

字节火山引擎发布其最新的视频生成模型Seedance 1.0 Pro 

在Artificial Analysis榜单上，Seedance 1.0 pro在文生视频、图生视频两个任务的表现均排名首位，那么其能力究竟如何呢？


该模型具备以下特点：

1. 无缝多镜头叙事：可以在一条视频中自然切换多个镜头，支持远景、中景、近景组合，叙事流畅不突兀。在一段视频中可以自动实现多个连续镜头的内容编排。 也就是多个镜头来回切换 像多机位一样 同时角色外观、姿势、视角保持连续。
    
2. 多动作&自由运镜：模型能识别多主体、生成复杂动作和情绪变化，同时支持灵活运镜（比如推拉摇移），画面更像“人拍的”，支持复杂叙事结构的视频任务模拟。
    
3. 画面稳定+真实美感：通过后训练调优+多维反馈学习，提升了画面结构稳定性，动态自然、崩溃率低，还能生成多种风格（写实、动漫、影视、广告等）的视频内容。具备高“动作物理一致性”与“空间连续性” ，既可模拟微表情（例如嘴角微笑） 也能生成大幅度动作（如奔跑、跳跃、镜头快速切换）

不仅生成质量领先，Seedance在性价比上的表现更是“离谱”：

- 生成一条5秒1080P视频只需3.67元
    
- 一万块预算，能做2700条1080P短视频
    
- 如果降到780P，用Seedance Lite，能做近9700条
    
在实际应用场景方面，Seedance已经展现出强大的落地能力：

- 电商：可自动生成多角色、多角度的产品广告视频

- 影视：快速验证分镜，节省前期筹备成本
    
- 游戏：玩家可自定义角色+剧情，生成定制视频


总体来说：

Seedance 1.0 pro 确实有了非常大的进步，尤其是在镜头控制、真实世界物理规律模拟上和指令跟随方面大幅提升，出片率很高，效果也很好。

和Veo 3在画面、镜头、美学、物理规律模拟方面不相上下，但是由于Veo 3有声音加持，一下就拉出了新的高度。不是一个产品级别了，不过已经非常厉害了

Seedance 1.0 pro 现在可以在即梦上体验，选择即梦3.0Pro 就是这个模型。



### 谷歌 Veo 3

作为首个支持音画同步的模型，其生成视频可自动匹配口型、环境音效及背景音乐，甚至能处理RAP等复杂语音。Veo 3不仅能生成高质量的4K视频画面，更能理解视频中的原始像素信息，自动生成与画面完美同步的对话、音效和背景音乐。目前通过Gemini应用向Ultra订阅用户开放。

这一突破的核心在于谷歌DeepMind团队开发的V2A（Video-to-Audio）技术。该技术能够将视频的视觉信息编码为语义信号，结合文本提示输入扩散模型，从而生成与画面匹配的完整音轨。简单来说，V2A就是Veo 3的”耳朵”和”声带”，让AI真正理解了视听结合的艺术。

技术亮点包括：

- 长视频生成：通过首尾帧控制生成1分钟以上连贯叙事。

- 多风格适配：从烹饪节目到科幻场景均能实现电影级质感。


核心技术能力解析：从画面到声音的全方位升级

(1) 视觉生成能力的飞跃
   
Veo 3在视觉生成方面实现了多项重大突破：

| 技术特性       | 具体表现                                           | 对比优势                                      |
|--------------|--------------------------------------------------|---------------------------------------------|
| 4K原生输出    | 支持原生4K分辨率，接近专业摄影机画质                   | 画面细节丰富，可无缝嵌入真实拍摄片段                  |
| 物理一致性    | 精确模拟光照逻辑、材质质感、运动物理                   | 大幅减少不合理的物理现象                          |
| 提示词理解    | 支持复杂自然语言描述和专业导演指令                     | 能理解镜头运动、情绪基调、构图细节                    |
| 场景连贯性    | 保持角色与背景的逻辑一致性                           | 支持复杂多人互动和动态叙事                        |

(2) 音频生成的革命性创新

Veo 3最令人惊艳的特性是其音频生成能力

- 对话生成：能够根据画面内容自动生成符合情境的人物对白
- 口型同步：实现了接近完美的唇音对齐效果
- 环境音效：自动生成各种环境声音，如脚步声、风声、机械声等
- 背景音乐：根据场景氛围自动配置合适的背景音乐
- 情绪渲染：能够捕捉画面情绪并生成相应的氛围音效



### OpenAI Sora

早期标杆模型，支持1分钟单镜头视频生成，擅长复杂运镜与光影效果，但暂未开放公众使用。

关于Sora的技术栈解读，详见链接🔗：https://zhuanlan.zhihu.com/p/686141310



## 二、国产模型生态

### 快手可灵 2.1
专注短视频生成，ARR突破1亿美元，支持方言配音和直播数字人。商用成本与Seedance接近，5秒视频3.5元。

快手可灵 2.1 上新功能：

- 高效灵动，超高性价比，支持标准（720p）/高品质（1080p）两种模式，标准模式低至 20 灵感值/5s、高品质模式 35 灵感值/5s。
- 升级为 1080p 高清画质，卓越运动表现，更强语义响应。


### 腾讯 Hunyuan
实现角色一致性控制，可指定人物形象生成连贯动作视频，适用于虚拟主播和广告定制。

腾讯Hunyuan是腾讯公司推出的多模态AI模型体系，涵盖视频生成、3D建模、自然语言处理等领域，其技术架构和应用生态具有以下核心特点：

视频生成模型HunyuanVideo

- 基于DiT（Diffusion Transformer）架构，支持文生视频功能，参数规模达130亿。

- 具备原生转场和切镜能力，生成视频分辨率为1080P，单次生成时长为5秒，未来计划支持更高分辨率及图生视频功能。

- 通过“提示词重写”优化生成效果：普通模式优化语义贴合度，高级模式增强光影、构图与摄像机运动细节。


3D生成模型Hunyuan3D

- 支持文/图生3D模型，采用两阶段架构：多视角扩散模型生成多视角图像，前馈重建网络转换为3D资产。
- 轻量版（约10秒生成）与标准版（约25秒生成）适应不同硬件需求，生成模型兼容主流格式（如Mesh、PBR材质）。
- 2025年升级至2.0版本，引入FlashVDM加速框架，生成速度提升30倍（从30秒缩短至1秒）。

语言模型Hunyuan-Large与T1

- Hunyuan-Large：3890亿参数的MoE（混合专家）模型，激活参数520亿，支持256K长文本处理，数学推理能力在GSM8K、MATH等数据集表现突出。

- Hunyuan-T1：专为推理优化的模型，采用混合Mamba-Transformer架构，首字响应速度达秒级，适用于实时对话与长文本处理。


### 阿里通义万相
2024年免费开放文生视频功能，支持国风元素生成，在电商与影视领域应用广泛。

通义万相 2.1 提供以下主要功能：

- 支持消费级 GPU：T2V-1.3B 型号仅需 8.19 GB VRAM，几乎兼容所有消费级 GPU。它可以在大约 5 分钟内在 RTX 4090 上生成 4 秒的 4P 视频（无需量化等优化技术）。它的性能甚至可以与一些闭源模型相媲美。（8.19G显存，这点普通人也能用了）

- 多任务：Wan2.1 擅长文本到视频、图像到视频、视频编辑、文本到图像和视频到音频，推动了视频生成领域的发展。

- 视觉文本生成：Wan2.1 是第一个能够同时生成中英文文本的视频模型，具有强大的文本生成功能，增强了其实际应用。

- 强大的视频 VAE：Wan-VAE 提供卓越的效率和性能，对任意长度的 1080P 视频进行编码和解码，同时保留时间信息，使其成为视频和图像生成的理想基础。

## 三、开源与学术突破

### 清华团队 Magi-1
首个开源自回归视频模型，支持无限长度扩展和物理规律模拟，框架基于Diffusion Transformer。

Magi-1，首个实现顶级画质输出的自回归视频生成模型，模型权重、代码100%开源。它将视频生成卷到了新高度，大片级品质直接锁住大家的眼球。

其主打能力，一是无限长度扩展，实现跨时间的无缝连贯叙事。二是能将控制精确到每一“秒”，10s内自定义视频时长。

另外，Magi-1对物理规律也有更深度的理解，Physics-IQ基准测试56.02%，大幅领先一众顶流。

这匹“黑马”来自中国团队Sand.ai，中文名听着有点萌叫三呆科技，实力却不容小觑。创始人曹越，博士毕业于清华大学软件学院，2018年获清华大学特等奖学金，此外还是光年之外联合创始人。

在Magi-1公布的整整61页技术报告中，还详细介绍了创新的注意力改进和推理基础设施设计，给人一种视频版DeepSeek的感觉。

Magi-1整体架构基于Diffusion Transformer，采用Flow-Matching作为训练目标。其最大的特点是不把视频当成一个整体去生成，而是通过自回归去噪方式预测固定长度的视频片段（chunk），每个片段固定为24帧。

在注意力机制上，也是提出了多项创新，包括：

- Block-Causal Attention

- Parallel Attention Block
  
- QK-Norm和GQA

- Flex-Flash-Attention

- 计算负载均衡

- 零冗余通信原语

- 自适应多阶段重叠


### LTX-Video
消费级显卡可部署的实时生成模型，速度比主流模型快30倍，支持720P高清输出。

LTX-Video 在 2025年5月14日发布的新版本中，通过蒸馏与FP8量化技术实现了 高质量视频生成与极低显存占用的统一，可在消费级显卡上实现720p高清视频的实时生成，为创意内容生产与边缘AI部署带来革命性突破。

13B蒸馏模型（v0.9.7-distilled）全新发布

- 生成速度极快：在 H100 上 10 秒生成高清视频，3 秒内输出低清预览

- 训练优化：无需使用 Classifier-Free Guidance (CFG) 与 Spatiotemporal Guidance (STG)

- 高效采样：推荐仅使用 8 步扩散 即可生成高质量视频

全新发布：FP8 量化模型（13B蒸馏版）

- 模型名：ltxv-13b-0.9.7-distilled-fp8

- 显存需求仅约 8GB，支持部署在 RTX 4060 等中端显卡

- 实测表现：在 RTX 4060 上可在 1 分钟内生成 121 帧 720×480 分辨率视频

- 适配 ComfyUI 官方工作流，可视化推理体验良好

- 无精度损失，同时获得高达 3× 的推理加速效果

其他相关模型与功能更新

- 发布 ltxv-13b-0.9.7-fp8（13B原始模型的FP8量化版本）

- 发布 ltxv-13b-0.9.7-distilled-lora128（仅需 1GB 显存，适合微调扩展）

- 更新多尺度渲染管线，支持更平衡的画质/速度切换

- 强化提示词适配能力，支持自动 Prompt Enhancement 机制


### BAGEL（字节跳动）
端到端多模态模型，具备“推理链”机制，可完成图像编辑、未来帧预测等复杂任务。

混合专家多模态模型，支持视觉理解，文本到图像生成，图像编辑，并且思考模式可以选择开启。官方说要比 Qwen2.5-VL 和 InternVL-2.5 表现好。这个模型本身是基于 Qwen2.5-7B-Instruct 和 siglip-so400m-14-980-flash-attn2-navit 模型微调的，并使用 FLUX.1-schnell VAE 模型。

技术架构与功能

- 统一多模态框架：BAGEL突破传统多模态模型分工模式，将视觉理解、图像生成、视频分析等功能整合于单一模型，支持输入文本、图像、视频等多模态数据，并能输出多模态内容（如带推理步骤的图文混合结果）。例如用户可通过多轮对话实现从生成图像到修改风格的无缝衔接。

- 推理链机制（Reasoning Chain）：模型在执行生成或编辑任务前会进行显式逻辑推理，如修改图像中的物体颜色时，先通过文字分解步骤：定位目标物体、分析当前色彩结构、规划修改路径，再执行操作。这种机制提升了对复杂指令的响应精度与控制力。

- 世界建模能力：支持多视角合成（如360°展示物体）、视频导航（从单张图像推演场景变化）及未来帧预测，例如根据静态地图生成动态路径视角，或预测视频后续画面。



## 四、技术趋势与挑战

### 生成质量提升
物理一致性（如水流、火焰模拟）和指令遵循能力成为竞争焦点，头部模型抽卡失败率降至5%以下。

### 成本降低
通过模型蒸馏与FP8量化技术，部分开源模型显存占用从24GB降至8GB，推理速度提升3倍。


### 伦理争议
深度伪造风险引发监管关注，中国网信办已开展AI生成内容专项整治，要求添加水印标识。


当前视频生成技术正从创意工具向产业基础设施演进，未来可能成为构建数字世界模拟器（World Model）的关键组件。


