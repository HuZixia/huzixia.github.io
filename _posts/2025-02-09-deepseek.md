---
layout: post
title: Operate｜DeepSeek-R1：LLMs 通过强化学习激励推理能力
categories: [Operate]
description: DeepSeek-R1：LLMs 通过强化学习激励推理能力
keywords: DeepSeek, DeepSeek-R1, RL, 强化学习, 蒸馏
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
topmost: false
---


本文翻译自 DeepSeek-R1 官方技术报告，主要包括的内容如下：引言、方法、实验、讨论、结论、局限性和未来工作。其中方法包括DeepSeek-R1-Zero：基础模型上的强化学习、DeepSeek-R1： 使用冷启动进行强化学习、蒸馏：赋予小模型推理能力。实验包括DeepSeek-R1 评估基准、蒸馏模型评估模型等。


原文链接：https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf。


- [摘要](#摘要)
- [1. 引言](#1-引言)
  - [1.1. 贡献](#11-贡献)
  - [1.2. 评估结果总结](#12-评估结果总结)
- [2. 方法](#2-方法)
  - [2.1. 概述](#21-概述)
  - [2.2. DeepSeek-R1-Zero：基础模型上的强化学习](#22-deepseek-r1-zero基础模型上的强化学习)
    - [2.2.1. 强化学习算法](#221-强化学习算法)
    - [2.2.2. 奖励建模](#222-奖励建模)
    - [2.2.3. 训练模板](#223-训练模板)
    - [2.2.4. DeepSeek-R1-Zero 的性能、自进化过程和顿悟时刻](#224-deepseek-r1-zero-的性能自进化过程和顿悟时刻)
  - [2.3. DeepSeek-R1： 使用冷启动进行强化学习](#23-deepseek-r1-使用冷启动进行强化学习)
    - [2.3.1. 冷启动](#231-冷启动)
    - [2.3.2. 面向推理的强化学习](#232-面向推理的强化学习)
    - [2.3.3. 抑制采样和监督微调](#233-抑制采样和监督微调)
    - [2.3.4. 适用于所有场景的强化学习](#234-适用于所有场景的强化学习)
  - [2.4. 蒸馏：赋予小模型推理能力](#24-蒸馏赋予小模型推理能力)
- [3. 实验](#3-实验)
  - [3.1. DeepSeek-R1 评估基准](#31-deepseek-r1-评估基准)
  - [3.2. 蒸馏模型评估模型](#32-蒸馏模型评估模型)
- [4. 讨论](#4-讨论)
  - [4.1. 蒸馏 vs. 强化学习](#41-蒸馏-vs-强化学习)
  - [4.2. 尝试失败](#42-尝试失败)
- [5. 结论、局限性和未来工作](#5-结论局限性和未来工作)


## 摘要

介绍了我们的第一代推理模型 DeepSeek-R1-Zero 和 DeepSeek-R1。DeepSeek-R1-Zero 是一种通过大规模强化学习 （RL） 训练的模型，没有作为初步步骤进行监督微调 （SFT），展示了卓越的推理能力。通过 RL，DeepSeek-R1-Zero 自然而然地出现了许多强大而有趣的推理行为。然而，它遇到了可读性差和语言混合等挑战。为了解决这些问题并进一步提高推理性能，我们引入了 DeepSeek-R1，它在 RL 之前结合了多阶段训练和冷启动数据。DeepSeekR1 在推理任务上实现了与 OpenAI-o1-1217 相当的性能。 为了支持研究社区，我们开源了 DeepSeek-R1-Zero、DeepSeek-R1 以及基于 Qwen 和 Llama 从 DeepSeek-R1 中提炼出来的六个密集模型（1.5B、7B、8B、14B、32B、70B）。


<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek01.png" style="margin-left: 0px" width="800px">


## 1. 引言

近年来，大型语言模型（LLMs）一直在经历快速迭代和进化（Anthropic，2024 年;谷歌，2024 年;OpenAI，2024a），逐步缩小与通用人工智能 （AGI） 的差距。


最近，后训练已成为完整训练管道的重要组成部分。它已被证明可以提高推理任务的准确性，与社会价值观保持一致，并适应用户偏好，同时需要相对最少的计算资源来对抗预训练。在推理能力方面，OpenAI 的 o1 （OpenAI， 2024b） 系列模型率先通过增加思维链推理过程的长度来引入推理时间缩放。这种方法在各种推理任务中取得了显着改进，例如数学、编码和科学推理。然而，有效测试时间缩放的挑战仍然是研究界的一个悬而未决的问题。之前的几项工作已经探索了各种方法，包括基于过程的奖励模型（Lightman等人，2023 年;Uesato 等人，2022 年; Wang et al.， 2023）、强化学习（Kumar et al.， 2024）和蒙特卡洛树搜索和光束搜索等搜索算法（Feng et al.， 2024;Trinh et al.， 2024;Xin et al.， 2024）。然而，这些方法都没有达到与 OpenAI 的 o1 系列模型相当的一般推理性能。

在本文中，我们迈出了使用纯强化学习 （RL） 提高语言模型推理能力的第一步。我们的目标是探索在没有任何监督数据的情况下开发推理能力的潜力LLMs，专注于通过纯强化学习过程进行自我进化。具体来说，我们使用 DeepSeek-V3-Base 作为基础模型，并使用 GRPO （Shao et al.， 2024） 作为 RL 框架来提高模型在推理中的性能。在训练过程中，DeepSeek-R1-Zero 自然而然地出现了许多强大而有趣的推理行为。经过数千次 RL 步骤，DeepSeek-R1-Zero 在推理基准测试中表现出卓越的性能。例如，AIME 2024 的 pass@1 分从 15.6% 提高到 71.0%，在多数投票的情况下，分数进一步提高到 86.7%，与 OpenAI-o1-0912 的性能相当。

然而，DeepSeek-R1-Zero 遇到了可读性差和语言混合等挑战。为了解决这些问题并进一步提高推理性能，我们引入了 DeepSeek-R1，它结合了少量冷启动数据和多阶段训练管道。具体来说，我们首先收集数千个冷启动数据，以微调 DeepSeek-V3-Base 模型。在此之后，我们执行面向推理的 RL，如 DeepSeek-R1Zero 。在 RL 过程接近收敛后，我们通过对 RL 检查点的拒绝采样创建新的 SFT 数据，并结合来自 DeepSeek-V3 的写作、事实 QA 和自我认知等领域的监督数据，然后重新训练 DeepSeek-V3-Base 模型。在使用新数据进行微调后，检查点将经历一个额外的 RL 过程，同时考虑所有场景的提示。 经过这些步骤，我们获得了一个名为 DeepSeek-R1 的检查点，它的性能与 OpenAI-o1-1217 相当。

我们进一步探索了从 DeepSeek-R1 到更小的密集模型的蒸馏。使用 Qwen2.532B （Qwen， 2024b） 作为基础模型，DeepSeek-R1 的直接蒸馏优于对其应用 RL。这表明，大型基础模型发现的推理模式对于提高推理能力至关重要。我们开源了提炼的 Qwen 和 Llama （Dubey et al.， 2024） 系列。值得注意的是，我们提炼的 14B 模型的性能大大优于最先进的开源 QwQ-32B-Preview （Qwen， 2024a），提炼的 32B 和 70B 模型在密集模型中的推理基准上创下了新纪录。


### 1.1. 贡献

**训练后：在基础模型上进行大规模强化学习**

我们直接将 RL 应用于基础模型，而无需依赖监督微调 （SFT） 作为初步步骤。这种方法允许模型探索解决复杂问题的思维链 （CoT），从而开发 DeepSeek-R1-Zero。DeepSeekR1-Zero 展示了自我验证、反射和生成长 CoT 等功能，标志着研究界的一个重要里程碑。值得注意的是，这是第一项验证LLMs推理能力可以纯粹通过 RL 激励的开放研究，而无需 SFT。这一突破为该领域的未来发展铺平了道路。

我们介绍了开发 DeepSeek-R1 的管道。该管道包含两个 RL 阶段，旨在发现改进的推理模式并符合人类偏好，以及两个 SFT 阶段，作为模型推理和非推理能力的种子。我们相信，该管道将通过创建更好的模型使行业受益。


**蒸馏：较小的模型也可以很强大**

我们证明，较大模型的推理模式可以提炼成更小的模型，与通过 RL 在小型模型上发现的推理模式相比，性能更好。开源 DeepSeek-R1 及其 API 将使研究社区受益，以便将来提炼出更好的更小模型。

使用 DeepSeek-R1 生成的推理数据，我们微调了研究界广泛使用的几个密集模型。评估结果表明，提炼的较小密集模型在基准测试中表现非常出色。DeepSeekR1-Distill-Qwen-7B 在 AIME 2024 上取得了 55.5% 的成绩，超过了 QwQ-32B-Preview。此外，DeepSeek-R1-Distill-Qwen-32B 在 AIME 2024 上的得分为 72.6%，在 MATH-500 上为 94.3%，在 LiveCodeBench 上为 57.2%。这些结果明显优于以前的开源模型，可与 o1-mini 相媲美。我们将基于 Qwen2.5 和 Llama3 系列的 1.5B 、 7B 、 8B 、 14B 、 32B 和 70B 检查点开源给社区。


### 1.2. 评估结果总结

推理任务：

（1） DeepSeek-R1 在 AIME 2024 上取得了 79.8% Pass@1分，略高于 OpenAI-o1-1217。在 MATH-500 上，它获得了令人印象深刻的 97.3% 分数，与 OpenAI-o1-1217 相当，性能明显优于其他模型。

（2） 在与编码相关的任务上，DeepSeek-R1 在代码竞赛任务中表现出专家级水平，因为它在 Codeforces 上获得了 2,029 的 Elo 评分，超过了 96.3% 的人类参与者。对于与工程相关的任务，DeepSeek-R1 的性能略好于 DeepSeek-V3，这可以帮助开发人员完成实际任务。


知识：

在 MMLU、MMLU-Pro 和 GPQA Diamond 等基准测试中，DeepSeekR1 取得了出色的结果，显著优于 DeepSeek-V3，在 MMLU 上得分为 84.0%，在 GPQA Diamond 上得分为 71.5%。虽然 DeepSeek-R1 在这些基准测试中的性能略低于 OpenAI-o1-1217，但超越了其他闭源模型，展示了其在教育任务方面的竞争优势。在事实基准测试 SimpleQA 中，DeepSeek-R1 的性能优于 DeepSeek-V3，展示了其处理基于事实的查询的能力。在此基准测试中，OpenAI-o1 超过了 4o，也观察到了类似的趋势。


其他：

DeepSeek-R1 在广泛的任务中也表现出色，包括创意写作、一般问答、编辑、总结等。它在 AlpacaEval 2.0 上取得了令人印象深刻的 87.6% 的长度控制胜率，在 ArenaHard 上取得了 92.3% 的胜率，展示了其智能处理非考试导向查询的强大能力。

此外，DeepSeek-R1 在需要长期上下文理解的任务上表现出出色的性能，在长期上下文基准测试中大大优于 DeepSeek-V3。


## 2. 方法

### 2.1. 概述

以前的工作严重依赖大量的监督数据来提高模型性能。在这项研究中，我们证明，即使不使用监督微调 （SFT） 作为冷启动，也可以通过大规模强化学习 （RL） 显著提高推理能力。此外，通过包含少量的冷启动数据，可以进一步提高性能。在以下部分中，我们将介绍：

（1） DeepSeek-R1-Zero，将 RL 直接应用于基础模型，无需任何 SFT 数据，以及 
（2） DeepSeek-R1，从具有数千个长思维链 （CoT） 示例的微调检查点开始应用 RL。
（3） 将 DeepSeek-R1 的推理能力提炼到小型密集模型中。


### 2.2. DeepSeek-R1-Zero：基础模型上的强化学习

强化学习在推理任务中已经显示出显著的有效性，我们之前的工作证明了这一点（Shao et al.， 2024;Wang et al.， 2023）。然而，这些工作严重依赖于监督数据，而这些数据的收集非常耗时。在本节中，我们探讨了在没有任何监督数据的情况下发展推理能力的潜力LLMs，重点关注它们通过纯粹的强化学习过程进行自我进化。我们首先简要概述了我们的 RL 算法，然后介绍了一些令人兴奋的结果，并希望这能为社区提供有价值的见解。


#### 2.2.1. 强化学习算法

群体相对策略优化为了节省RL的训练成本，我们采用了群体相对策略优化（GRPO）（Shao et al.， 2024），它放弃了通常与策略模型大小相同的批评者模型，而是从群体分数中估计基线。具体来说，对于每个问题q，GRPO从旧的策略π中抽取一组输出{o， o， · · · · o}，然后优化策略模型 π通过最大化以下目标：


<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek02.png" style="margin-left: 0px" width="800px">


<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek03.png" style="margin-left: 0px" width="800px">


#### 2.2.2. 奖励建模

奖励是训练信号的来源，它决定了 RL 的优化方向。为了训练 DeepSeek-R1-Zero，我们采用了基于规则的奖励系统，主要由两类奖励组成：


准确率奖励：

准确率奖励模型评估响应是否正确。例如，对于具有确定性结果的数学问题，模型需要以指定格式（例如，在框内）提供最终答案，从而实现可靠的基于规则的正确性验证。同样，对于 LeetCode 问题，编译器可用于根据预定义的测试用例生成反馈。

格式奖励：

除了准确率奖励模型外，我们还采用了格式奖励模型，该模型强制模型将其思考过程置于 '' 和 '' 标签之间。


在开发 DeepSeek-R1-Zero 时，我们没有应用结果或过程神经奖励模型，因为我们发现神经奖励模型在大规模强化学习过程中可能会遭受奖励黑客攻击，重新训练奖励模型需要额外的训练资源，并且使整个训练管道复杂化。


#### 2.2.3. 训练模板

为了训练 DeepSeek-R1-Zero，我们首先设计一个简单的模板，指导基本模型遵守我们指定的指令。如表 1 所示，此模板要求 DeepSeek-R1-Zero 首先生成推理过程，然后生成最终答案。我们有意将约束限制在这种结构格式上，避免任何特定于内容的偏见，例如强制进行反思推理或促进特定的问题解决策略，以确保我们能够在 RL 过程中准确观察模型的自然进展。


#### 2.2.4. DeepSeek-R1-Zero 的性能、自进化过程和顿悟时刻

DeepSeek-R1-Zero 的性能图 2 描绘了 DeepSeekR1-Zero 在整个 RL 训练过程中在 AIME 2024 基准测试中的性能轨迹。如图所示，随着 RL 训练的推进，DeepSeek-R1-Zero 的性能得到了稳定和一致的增强。值得注意的是，AIME 2024 的平均 pass@1 分数显示出显着提高，从最初的 15.6% 跃升到令人印象深刻的 71.0%，达到了与 OpenAI-o1-0912 相当的性能水平。这一显著改进凸显了我们的 RL 算法在随着时间的推移优化模型性能方面的功效。

表 2 提供了 DeepSeek-R1-Zero 和 OpenAI 的 o1-0912 模型在各种推理相关基准上的比较分析。研究结果表明，RL 赋予DeepSeek-R1-Zero 无需任何监督微调数据即可获得强大的推理能力。这是一项值得注意的成就，因为它强调了该模型仅通过 RL 进行有效学习和概括的能力。此外，DeepSeekR1-Zero 的性能可以通过应用多数投票进一步增强。例如，当在 AIME 基准测试中采用多数投票时，DeepSeek-R1-Zero 的性能从 71.0% 升级到 86.7%，从而超过了 OpenAI-o1-0912 的性能。DeepSeek-R1-Zero 能够在有和没有多数投票的情况下实现如此有竞争力的性能，这凸显了其强大的基础能力及其在推理任务中进一步发展的潜力。


<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek04.png" style="margin-left: 0px" width="800px">



**DeepSeek-R1-Zero 的自我进化过程**

DeepSeek-R1-Zero 的自我进化过程是一个引人入胜的演示，展示了 RL 如何驱动模型自主提高其推理能力。通过直接从基础模型启动 RL，我们可以密切监控模型的进度，而不受监督微调阶段的影响。这种方法清楚地展示了模型如何随时间演变，特别是在处理复杂推理任务的能力方面。


如图 3 所示，贯穿整个训练过程，DeepSeek-R1-Zero 的思考时间显示出持续的改善。这种改进不是外部调整的结果，而是模型内部的内在发展。DeepSeek-R1-Zero 通过利用扩展的测试时间计算，自然而然地获得了解决日益复杂的推理任务的能力。这种计算范围从生成数百到数千个推理令牌，使模型能够更深入地探索和完善其思维过程。

这种自我进化最引人注目的方面之一是，随着测试时计算的增加，复杂行为的出现。诸如反思（模型重新审视和重新评估其先前步骤）等行为以及探索解决问题的替代方法等行为都是自发产生的。这些行为没有明确编程，而是作为模型与强化学习环境交互的结果而出现的。这种自发的发展显著增强了 DeepSeek-R1-Zero 的推理能力，使其能够以更高的效率和准确性处理更具挑战性的任务。

<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek05.png" style="margin-left: 0px" width="800px">



**DeepSeek-R1-Zero 的顿悟时刻**

在 DeepSeek-R1-Zero 的训练过程中观察到的一个特别有趣的现象是“顿悟时刻”的出现。如表 3 所示，这个时刻发生在模型的中间版本中。在这个阶段，DeepSeek-R1-Zero 通过重新评估其初始方法，学会为问题分配更多的思考时间。这种行为不仅证明了模型不断增长的推理能力，也是强化学习如何导致意想不到的复杂结果的迷人例子。


这一刻不仅是模型的“顿悟时刻”，也是观察其行为的研究人员的“顿悟时刻”。它强调了强化学习的力量和美感：我们不是明确地教模型如何解决问题，而是简单地为它提供正确的激励，它就会自主开发先进的问题解决策略。“顿悟时刻”有力地提醒我们，RL 有可能在人工系统中解锁新的智能水平，为未来更加自主和自适应的模型铺平道路。

<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek06.png" style="margin-left: 0px" width="800px">


**DeepSeek-R1-Zero 的缺点**

尽管 DeepSeek-R1-Zero 表现出强大的推理能力，并自主发展出意想不到的强大推理行为，但它面临几个问题。例如，DeepSeek-R1-Zero 努力应对可读性差和语言混合等挑战。为了使推理过程更具可读性并与开放社区分享，我们探索了 DeepSeek-R1，这是一种利用 RL 和人类友好的冷启动数据的方法。



### 2.3. DeepSeek-R1： 使用冷启动进行强化学习


受到 DeepSeek-R1-Zero 的有希望结果的启发，自然而然地出现了两个问题：

1） 通过纳入少量高质量数据作为冷启动，是否可以进一步提高推理性能或加速收敛？

2） 我们如何训练一个用户友好的模型，该模型不仅产生清晰连贯的思维链 （CoT），而且还展示了强大的通用能力？

为了解决这些问题，我们设计了一个管道来训练 DeepSeek-R1。该管道由四个阶段组成，概述如下。


#### 2.3.1. 冷启动

与 DeepSeek-R1-Zero 不同，为了防止基础模型出现 RL 训练的早期不稳定冷启动阶段，对于 DeepSeek-R1，我们构建并收集少量的长 CoT 数据，以微调模型作为初始 RL 参与者。为了收集这些数据，我们探索了几种方法：以长 CoT 的 few-shot 提示为例，直接提示模型通过反射和验证生成详细的答案，以可读格式收集 DeepSeek-R1Zero 输出，并通过人工注释者进行后处理来提炼结果。

在这项工作中，我们收集了数千个冷启动数据，以微调 DeepSeek-V3-Base 作为 RL 的起点。与 DeepSeek-R1-Zero 相比，冷启动数据的优势包括：

可读性：

DeepSeek-R1-Zero 的一个关键限制是其内容通常不适合阅读。响应可能混合多种语言或缺少 markdown 格式来为用户突出显示答案。相比之下，在为 DeepSeek-R1 创建冷启动数据时，我们设计了一个可读模式，在每个响应的末尾包含一个摘要，并过滤掉对读者不友好的响应。在这里，我们将输出格式定义为 |special_token||special_token|<summary>，其中推理过程是查询的 CoT，摘要用于总结推理结果。</summary>


潜力：

通过使用人类先验仔细设计冷启动数据的模式，我们观察到与 DeepSeek-R1-Zero 相比性能更好。我们相信迭代训练是推理模型的更好方法。


#### 2.3.2. 面向推理的强化学习

在根据冷启动数据对 DeepSeek-V3-Base 进行微调后，我们应用了与 DeepSeek-R1-Zero 中相同的大规模强化学习训练过程。这个阶段的重点是增强模型的推理能力，特别是在推理密集型任务中，如编码、数学、科学和逻辑推理，这些任务涉及定义明确的问题和明确的解决方案。在训练过程中，我们观察到 CoT 经常表现出语言混合，特别是当 RL 提示涉及多种语言时。为了缓解语言混合问题，我们在 RL 训练期间引入了语言一致性奖励，其计算方式是 CoT 中目标语言单词的比例。尽管消融实验表明，这种对齐会导致模型的性能略有下降，但这种奖励与人类的偏好保持一致，使其更具可读性。最后，我们将推理任务的准确性和语言一致性的奖励结合起来，直接将它们相加，形成最终的奖励。 然后，我们在微调模型上应用 RL 训练，直到它在推理任务上实现收敛。


#### 2.3.3. 抑制采样和监督微调


当面向推理的 RL 收敛时，我们利用生成的检查点来收集 SFT（监督微调）数据，用于下一轮。与主要关注推理的初始冷启动数据不同，这个阶段整合了来自其他领域的数据，以增强模型在写作、角色扮演和其他通用任务方面的能力。具体来说，我们生成数据并微调模型，如下所述。


**推理数据**

我们通过从上述 RL 训练的检查点执行拒绝采样来策划推理提示并生成推理轨迹。在上一阶段，我们只包含了可以使用基于规则的奖励进行评估的数据。然而，在这个阶段，我们通过整合额外的数据来扩展数据集，其中一些数据通过使用生成奖励模型，将真实和模型预测输入到 DeepSeek-V3 中进行判断。此外，由于模型输出有时混乱且难以阅读，我们过滤掉了混合语言、长释义和代码块的思维链。对于每个提示，我们都会对多个响应进行采样，并只保留正确的响应。我们总共收集了大约 600k 个与推理相关的训练样本。


**非推理数据**

对于非推理数据，例如写作、事实 QA、自我认知和翻译，我们采用 DeepSeek-V3 管道并重用 DeepSeek-V3 的 SFT 数据集的部分。对于某些非推理任务，我们调用 DeepSeek-V3 来生成潜在的思维链，然后再通过提示回答问题。但是，对于更简单的查询，例如 “hello”，我们不会提供 CoT 作为响应。最后，我们总共收集了大约 200k 个与推理无关的训练样本。

我们使用上述约 800k 样本的精选数据集对 DeepSeek-V3-Base 进行了两个 epoch 的微调。


#### 2.3.4. 适用于所有场景的强化学习

为了进一步使模型与人类偏好保持一致，我们实施了二次强化学习阶段，旨在提高模型的有用性和无害性，同时完善其推理能力。具体来说，我们使用奖励信号和不同提示分布的组合来训练模型。对于推理数据，我们遵循 DeepSeek-R1-Zero 中概述的方法，该方法利用基于规则的奖励来指导数学、代码和逻辑推理领域的学习过程。对于一般数据，我们采用奖励模型来捕捉复杂和细微场景中的人类偏好。我们以 DeepSeek-V3 管道为基础，采用类似的偏好对和训练提示分布。为了提供帮助，我们只关注最终摘要，确保评估强调响应对用户的效用和相关性，同时最大限度地减少对底层推理过程的干扰。 对于无害性，我们会评估模型的整个响应，包括推理过程和摘要，以识别和减轻生成过程中可能出现的任何潜在风险、偏见或有害内容。最终，奖励信号和不同数据分布的集成使我们能够训练一个在推理方面表现出色的模型，同时优先考虑有用性和无害性。


### 2.4. 蒸馏：赋予小模型推理能力


为了让更高效的小型模型像 DeepSeek-R1 一样具有推理能力，我们使用 DeepSeek-R1 精选的 800k 样本，直接对 Qwen （Qwen， 2024b） 和 Llama （AI@Meta， 2024） 等开源模型进行了微调，详见 §2.3.3。我们的研究结果表明，这种简单的蒸馏方法显著提高了较小模型的推理能力。我们在这里使用的基本模型是 Qwen2.5-Math-1.5B、Qwen2.5-Math-7B、Qwen2.514B、Qwen2.5-32B、Llama-3.1-8B 和 Llama-3.3-70B-Instruct。我们选择 Llama-3.3 是因为它的推理能力略好于 Llama-3.1。


对于蒸馏模型，我们只应用 SFT，不包括 RL 阶段，即使合并 RL 可以大大提高模型性能。我们在这里的主要目标是证明蒸馏技术的有效性，将 RL 阶段的探索留给更广泛的研究社区。


## 3. 实验


**基准**

我们评估了 MMLU（Hendrycks 等人，2020 年）、MMLU-Redux（Gema 等人，2024 年）、MMLU-Pro（Wang 等人，2024 年）、C-Eval（Huang 等人，2023 年）和 CMMLU（Li 等人，2023 年）、IFEval（周 等人，2023 年）、FRAMES（Krishna 等人，2024 年）、GPQA Diamond（Rein 等人，2023 年）、SimpleQA（OpenAI，2024c）、C-SimpleQA（He 等人，2024 年）、SWE-Bench 验证（OpenAI、2024d）、Aider、LiveCodeBench （Jain et al.， 2024） （2024-08 – 2025-01）、Codeforces、中国全国高中数学奥林匹克竞赛 （CNMO 2024） 和 2024 年美国数学邀请赛 （AIME 2024） （MAA， 2024）。除了标准基准外，我们还使用LLMs作为评委评估我们的开放式生成任务模型。具体来说，我们遵循 AlpacaEval 2.0（Dubois 等人，2024 年）和 Arena-Hard（Li 等人，2024 年）的原始配置，它们利用 GPT-4-Turbo-1106 作为评委进行成对比较。在这里，我们只将最终摘要提供给评估，以避免长度偏差。 对于蒸馏模型，我们报告了 AIME 2024、MATH-500、GPQA Diamond、Codeforces 和 LiveCodeBench 的代表性结果。


**评估提示**

按照 DeepSeek-V3 中的设置，使用 simpleevals 框架中的提示对 MMLU、DROP、GPQA Diamond 和 SimpleQA 等标准基准测试进行评估。对于 MMLU-Redux，我们在零镜头设置中采用 Zero-Eval 提示格式 （Lin， 2024）。在 MMLU-Pro、C-Eval 和 CLUE-WSC 方面，由于原始提示是 few-shot，我们将提示略微修改为 zero-shot 设置。few-shot 中的 CoT 可能会损害 DeepSeek-R1 的性能。其他数据集遵循其原始评估协议，其创建者提供默认提示。对于代码和数学基准测试，HumanEval-Mul 数据集涵盖了八种主流编程语言（Python、Java、C++、C#、JavaScript、TypeScript、PHP 和 Bash）。 LiveCodeBench 上的模型性能使用 CoT 格式进行评估，数据在 2024 年 8 月至 2025 年 1 月期间收集。Codeforces 数据集使用 10 个 Div.2 竞赛中的问题以及专家制作的测试用例进行评估，然后计算出竞争对手的预期评分和百分比。SWE-Bench 验证结果通过无代理框架获得（Xia et al.， 2024）。与 AIDER 相关的基准测试使用 “diff” 格式进行测量。DeepSeek-R1 输出的每个基准测试最多有 32,768 个令牌。


**基线**

我们针对几个强大的基线进行了全面评估，包括 DeepSeek-V3、Claude-Sonnet-3.5-1022、GPT-4o-0513、OpenAI-o1-mini 和 OpenAI-o1-1217。由于在中国大陆访问 OpenAI-o1-1217 API 具有挑战性，因此我们根据官方报告报告其性能。对于蒸馏模型，我们还比较了开源模型 QwQ-32B-Preview （Qwen， 2024a）。

评估设置我们将模型的最大生成长度设置为 32,768 个标记。我们发现，使用贪婪解码来评估长输出推理模型会导致更高的重复率和不同检查点的显着可变性。因此，我们默认pass@k评估（Chen等人，2021 年），并使用非零温度报告pass@1。具体来说，我们使用 0.6 的采样温度和 0.95 的 top-p 值为每个问题生成 k 个答案（通常在 4 到 64 之间，取决于测试集的大小）。然后Pass@1计算为


$
\text{pass@1} = \frac{1}{k} \sum_{i=1}^k p_i
$

其中 p表示第 i 个响应的正确性。这种方法提供了更可靠的性能估计。对于 AIME 2024，我们还报告了使用 64 个样本的共识（多数投票）结果（Wang et al.， 2022），表示为 cons@64。

### 3.1. DeepSeek-R1 评估基准


<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek07.png" style="margin-left: 0px" width="800px">



对于 MMLU、MMLU-Pro 和 GPQA Diamond 等面向教育的知识基准测试，DeepSeek-R1 与 DeepSeek-V3 相比表现出卓越的性能。这种改进主要归因于 STEM 相关问题的准确性提高，其中通过大规模强化学习实现了显著的收益。此外，DeepSeek-R1 在 FRAMES（一项长期上下文依赖的 QA 任务）上表现出色，展示了其强大的文档分析能力。这凸显了推理模型在 AI 驱动的搜索和数据分析任务中的潜力。在事实基准 SimpleQA 中，DeepSeek-R1 的性能优于 DeepSeek-V3，展示了其处理基于事实的查询的能力。在此基准上，OpenAI-o1 超过了 GPT-4o，也观察到了类似的趋势。 然而，DeepSeek-R1 在中国 SimpleQA 基准测试中的表现比 DeepSeek-V3 差，主要是因为它倾向于在安全 RL 之后拒绝回答某些查询。在没有安全 RL 的情况下，DeepSeek-R1 可以达到 70% 以上的准确率。


DeepSeek-R1 在 IF-Eval 上也提供了令人印象深刻的结果，IF-Eval 是一个基准，旨在评估模型遵循格式指令的能力。这些改进可以与在监督微调 （SFT） 和 RL 训练的最后阶段包含指令跟踪数据相关联。此外，在 AlpacaEval2.0 和 ArenaHard 上观察到了出色的性能，这表明 DeepSeek-R1 在编写任务和开放领域问答方面具有优势。DeepSeek-V3 的显著优异表现凸显了大规模强化学习（RL）的泛化优势，它不仅提升了推理能力，还提高了在多个领域的性能。此外，DeepSeek-R1 生成的摘要长度简洁明了，在 ArenaHard 上平均有 689 个标记，在 AlpacaEval 2.0 上平均有 2,218 个字符。这表明DeepSeek-R1 避免了在基于 GPT 的评估期间引入长度偏差，进一步巩固了其在多个任务中的稳健性。


在数学任务上，DeepSeek-R1 的性能与 OpenAI-o1-1217 相当，大大超过其他模型。在 LiveCodeBench 和 Codeforces 等编码算法任务中也观察到类似的趋势，在这些任务中，以推理为中心的模型在这些基准测试中占据主导地位。在面向工程的编码任务中，OpenAI-o1-1217 在 Aider 上的性能优于 DeepSeek-R1，但在 SWE 验证上实现了相当的性能。我们相信 DeepSeek-R1 的工程性能将在下一个版本中得到提升，因为目前相关 RL 训练数据的数量仍然非常有限。


### 3.2. 蒸馏模型评估模型


<img src="https://cdn.jsdelivr.net/gh/HuZixia/CloudGo/pictures/resources/deepseek/deepseek08.png" style="margin-left: 0px" width="800px">



如表 5 所示，只需蒸馏 DeepSeek-R1 的输出即可使高效的 DeepSeekR1-7B（即 DeepSeek-R1-Distill-Qwen-7B，缩写如下）全面优于 GPT-4o-0513 等非推理模型。DeepSeek-R1-14B 在所有评估指标上都超过了 QwQ-32BPreview，而 DeepSeek-R1-32B 和 DeepSeek-R1-70B 在大多数基准测试中明显超过 o1-mini。这些结果证明了蒸馏的巨大潜力。此外，我们发现将 RL 应用于这些蒸馏模型会产生显着的进一步收益。我们认为这值得进一步探索，因此在这里仅提供简单 SFT 蒸馏模型的结果。



## 4. 讨论

### 4.1. 蒸馏 vs. 强化学习

在 3.2 节中，我们可以看到，通过蒸馏 DeepSeek-R1，小模型可以取得令人印象深刻的结果。然而，仍然剩下一个问题：该模型能否通过论文中讨论的大规模 RL 训练实现可比的性能，而无需蒸馏？

为了回答这个问题，我们使用数学、代码和 STEM 数据在 Qwen-32B-Base 上进行了大规模的 RL 训练，训练了超过 10K 个步骤，产生了 DeepSeek-R1-Zero-Qwen-32B。实验结果如表 6 所示，表明 32B Base 模型在大规模 RL 训练实现了与 QwQ-32B-Preview 相当的性能。但是，从 DeepSeek-R1 中提炼出来的 DeepSeek-R1Distill-Qwen-32B 在所有基准测试中的性能明显优于 DeepSeek-R1-Zero-Qwen-32B。

因此，我们可以得出两个结论：首先，将更强大的模型提炼成更小的模型会产生极好的结果，而依赖于本文提到的大规模 RL 的较小模型需要巨大的计算能力，甚至可能无法达到蒸馏的性能。其次，虽然蒸馏策略既经济又有效，但超越智能界限可能仍然需要更强大的基础模型和更大规模的强化学习。


### 4.2. 尝试失败

在开发 DeepSeek-R1 的早期阶段，我们一路上也遇到了失败和挫折。我们在这里分享我们的失败经验以提供见解，但这并不意味着这些方法无法开发有效的推理模型。

**过程奖励模型 （PRM）**

PRM 是一种合理的方法，可以引导模型采用更好的方法来解决推理任务（Lightman et al.， 2023;Uesato et al.， 2022;Wang et al.， 2023）。然而，在实践中，PRM 有三个主要限制可能会阻碍其最终成功。首先，在一般推理中明确定义一个细粒度步骤是具有挑战性的。其次，确定当前的中间步骤是否正确是一项具有挑战性的任务。使用模型的自动注释可能不会产生令人满意的结果，而手动注释不利于扩大规模。第三，一旦引入基于模型的 PRM，就不可避免地导致奖励黑客攻击（Gao et al.， 2022），重新训练奖励模型需要额外的训练资源，并使整个训练管道复杂化。 总之，虽然 PRM 表现出对模型生成的前 N 个响应进行重新排序或协助引导搜索的良好能力（Snell et al.， 2024），但与它在大规模强化学习过程中引入的额外计算开销相比，它的优势是有限的。


**蒙特卡洛树搜索 （MCTS）**

受 AlphaGo（Silver 等人，2017b）和 AlphaZero（Silver 等人，2017a）的启发，我们探索了使用蒙特卡洛树搜索 （MCTS） 来增强测试时计算的可扩展性。这种方法涉及将答案分成更小的部分，以允许模型系统地探索解决方案空间。为了促进这一点，我们提示模型生成多个标签，这些标签对应于搜索所需的特定推理步骤。对于训练，我们首先使用收集到的提示，通过由预先训练的价值模型指导的 MCTS 找到答案。随后，我们使用生成的问答对来训练参与者模型和价值模型，迭代地改进该过程。


然而，这种方法在扩大训练规模时遇到了一些挑战。首先，与搜索空间相对明确的国际象棋不同，令牌生成呈现了一个指数级搜索空间。为了解决这个问题，我们为每个节点设置了最大扩展限制，但这可能会导致模型卡在局部最优状态。其次，价值模型直接影响生成质量，因为它指导搜索过程的每一步。训练细粒度的价值模型本身就很困难，这使得模型迭代改进具有挑战性。虽然 AlphaGo 的核心成功依赖于训练价值模型以逐步提高其性能，但由于令牌生成的复杂性，这一原则在我们的设置中被证明很难复制。

总之，虽然 MCTS 在与预先训练的值模型配对时可以提高推理过程中的性能，但通过自搜索迭代提高模型性能仍然是一项重大挑战。


## 5. 结论、局限性和未来工作

在这项工作中，我们分享了通过强化学习增强模型推理能力的历程。DeepSeek-R1-Zero 代表了一种纯 RL 方法，不依赖冷启动数据，在各种任务中实现了强大的性能。DeepSeek-R1 功能更强大，利用冷启动数据以及迭代 RL 微调。最终，DeepSeek-R1 在一系列任务上实现了与 OpenAI-o1-1217 相当的性能。

我们进一步探索了将推理能力提炼到小型密集模型。我们使用 DeepSeek-R1 作为教师模型来生成 800K 训练样本，并微调了几个小型密集模型。结果很有希望：DeepSeek-R1-Distill-Qwen-1.5B 在数学基准测试中优于 GPT-4o 和 Claude-3.5-Sonnet，在 AIME 上为 28.9%，在 MATH 上为 83.9%。其他密集模型也取得了令人印象深刻的结果，明显优于基于相同底层检查点的其他 instructiontuned 模型。


未来，我们计划投资于 DeepSeek-R1 的以下方向的研究。

**一般能力：**

目前，DeepSeek-R1 在函数调用、多轮次、复杂角色扮演和 JSON 输出等任务方面不如 DeepSeek-V3。展望未来，我们计划探索可以利用 CoT 时间来增强这些领域的任务。

**语言混合：**

DeepSeek-R1 目前针对中文和英文进行了优化，这可能会导致在处理其他语言的查询时出现语言混合问题。例如，DeepSeek-R1 可能会使用英语进行推理和响应，即使查询使用的是英语或中文以外的语言。我们的目标是在未来的更新中解决此限制。


**提示工程：**

在评估 DeepSeek-R1 时，我们观察到它对提示很敏感。小样本提示会持续降低其性能。因此，我们建议用户直接描述问题并使用零样本设置指定输出格式以获得最佳结果。

**软件工程任务：**

由于评估时间长，影响了 RL 流程的效率，大规模 RL 尚未在软件工程任务中得到广泛应用。因此，DeepSeek-R1 在软件工程基准测试中没有表现出比 DeepSeek-V3 有的巨大改进。未来的版本将通过对软件工程数据实施拒绝抽样或在 RL 过程中加入异步评估来解决这个问题，以提高效率。


