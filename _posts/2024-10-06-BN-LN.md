---
layout: post
title: Batch Norm & Layer Norm｜对比 Batch Norm 和 Layer Norm
categories: [Norm]
description: 对比 Batch Norm 和 Layer Norm
keywords: Batch Norm, Layer Norm
mermaid: false
sequence: false
flow: false
mathjax: false
mindmap: false
mindmap2: false
topmost: false
---

# Batch Norm 和 Layer Norm

对比 Batch Norm 和 Layer Norm，两者都是常用的归一化方法。其中 Batch Norm 对每个 mini-batch 的输入进行归一化，而 Layer Norm 对每个样本的输入进行归一化。Batch Norm 适用于 CNN、DNN 等，需要较大的 mini-batch 的场景，而 Layer Norm 适用于 RNN、LSTM、Transformer 等，尤其是小批量或单样本训练的场景。



- [Batch Norm 和 Layer Norm](#batch-norm-和-layer-norm)
  - [Batch Norm](#batch-norm)
  - [Layer Norm](#layer-norm)
  - [BN 和 LN 对比](#bn-和-ln-对比)


两者都是常用的归一化方法

## Batch Norm

- 对每个 **mini-batch** 的输入进行归一化
  - 计算均值和方差，对 **mini-batch 的每个特征**计算均值和方差
  - 归一化，使用均值和方差对输入进程标准化，计算均值和方差
    - $$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
  - 缩放和平移，通过学习参数 $$\gamma$$ 和 $$\beta$$ 进行缩放和平移
    - $$y_i = \gamma \hat{x}_i + \beta$$

## Layer Norm

- 对 **每个样本** 的输入进行归一化
  - 计算均值和方差，对 **每个样本 的所有特征**计算均值和方差
  - 归一化，使用均值和方差对输入进程标准化，计算均值和方差
    - $$\hat{x}_i = \frac{x_i - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}}$$
  - 缩放和平移，通过学习参数 $$\gamma$$ 和 $$\beta$$ 进行缩放和平移
    - $$y_i = \gamma \hat{x}_i + \beta$$

## BN 和 LN 对比

|       特性        |                       Batch Norm                       |                            Layer Norm                            |
| :---------------: | :----------------------------------------------------: | :--------------------------------------------------------------: |
|    归一化维度     |                 mini-batch 的每个特征                  |                       每个样本 的所有特征                        |
| 依赖于 mini-batch |                         是                           |                                否                                |
|     适用场景      |     适用于CNN、DNN等，需要较大的 mini-batch 的场景     | 适用于 RNN、LSTM、Transformer 等，尤其是小批量或单样本训练的场景 |
|     计算开销      |            对大批量训练效果较好，计算效率高            |                 对每个样本进行计算，计算开销较大                 |
|     主要优点      |   加速训练过程，提高泛化能力，减少对初始权重的敏感性   |             不依赖批量大小，适用于序列模型和在线学习             |
|    主要局限性     | 对小批量训练效果不佳，在序列模型和在线学习中表现不理解 |     在某些情况下训练速度可能较慢，需要计算每个样本的所有特征     |


[哔站视频讲解链接](https://www.bilibili.com/video/BV1gD1oYNEiQ/?vd_source=53eae980171553eea5dcb3bf040179f3)



